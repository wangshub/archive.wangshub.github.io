<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on 🍉 神奇的战士</title><link>https://wangshub.github.io/tags/reinforcement-learning/</link><description>Recent content in Reinforcement Learning on 🍉 神奇的战士</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 28 Mar 2020 11:26:03 +0000</lastBuildDate><atom:link href="https://wangshub.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>如何用深度强化学习模拟炒股</title><link>https://wangshub.github.io/posts/drl-stock/</link><pubDate>Sat, 28 Mar 2020 11:26:03 +0000</pubDate><guid>https://wangshub.github.io/posts/drl-stock/</guid><description>💡 初衷 最近一段时间，受到新冠疫情的影响，股市接连下跌，作为一棵小白菜兼小韭菜，竟然产生了抄底的大胆想法，拿出仅存的一点私房钱梭哈了一把。 第二</description></item><item><title>Pybulet Gym 源码解析：双足机器人模型 HumanoidPyBulletEnv-v0</title><link>https://wangshub.github.io/posts/pybulet-gym%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E5%8F%8C%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%A8%A1%E5%9E%8Bhumanoidpybulletenv-v0/</link><pubDate>Fri, 23 Aug 2019 16:19:36 +0000</pubDate><guid>https://wangshub.github.io/posts/pybulet-gym%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E5%8F%8C%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%A8%A1%E5%9E%8Bhumanoidpybulletenv-v0/</guid><description>OpenAI gym 是当前使用最为广泛的用于研究强化学习的工具箱，但 Gym 的物理仿真环境使用的是 Mujoco，不开源且收费，这一点一直被人诟病。而 Pybullet-gym 是对 Openai Gym Mujoco 环</description></item><item><title>OpenAI Gym 源码阅读:创建自定义强化学习环境</title><link>https://wangshub.github.io/posts/openai-gym-%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</link><pubDate>Wed, 21 Aug 2019 16:24:35 +0000</pubDate><guid>https://wangshub.github.io/posts/openai-gym-%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</guid><description>OpenAI Gym 源码阅读：创建自定义强化学习环境 Gym 介绍 Gym 是一套开发强化学习算法的工具箱，包含了一系列内置的环境，结合强化学习算法就可以对内置的环境进行求</description></item><item><title>Vanilla_Policy_Gradient</title><link>https://wangshub.github.io/posts/vanilla-policy-gradient/</link><pubDate>Tue, 29 Jan 2019 16:48:53 +0000</pubDate><guid>https://wangshub.github.io/posts/vanilla-policy-gradient/</guid><description>强化学习 Vanilla Policy Gradient 令 $\pi _ { \theta }$ 表示参数为 $\theta$ 的策略，$J \left( \pi _ { \theta } \right)$ 表示策略 $\pi _ { \theta }$ 的返回值，则优化函数梯度为 $$\nabla _ { \theta } J \left( \pi _ { \theta } \right) = \underset {</description></item><item><title>MLPGaussianPolicy</title><link>https://wangshub.github.io/posts/mlpgaussianpolicy/</link><pubDate>Mon, 14 Jan 2019 19:42:41 +0000</pubDate><guid>https://wangshub.github.io/posts/mlpgaussianpolicy/</guid><description>MLP Gaussian policy Looks like a Gaussian policy whose mean and std are outputs of a neural network. 参考文献 https://arxiv.org/pdf/1502.05477.pdf</description></item><item><title>强化学习随机策略之高斯似然数原理与代码实现</title><link>https://wangshub.github.io/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%9A%8F%E6%9C%BA%E7%AD%96%E7%95%A5%E4%B9%8B%E9%AB%98%E6%96%AF%E4%BC%BC%E7%84%B6%E6%95%B0%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</link><pubDate>Sat, 12 Jan 2019 14:47:50 +0000</pubDate><guid>https://wangshub.github.io/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%9A%8F%E6%9C%BA%E7%AD%96%E7%95%A5%E4%B9%8B%E9%AB%98%E6%96%AF%E4%BC%BC%E7%84%B6%E6%95%B0%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</guid><description>强化学习随机策略之高斯似然数原理与代码实现 一、原理介绍 使用随机策略有两个关键点 从策略当中进行采样，获得动作 $a$ (Action) 计算特定动作的似然数 $\log \pi _ { \theta</description></item></channel></rss>