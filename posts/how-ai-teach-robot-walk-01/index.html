<!DOCTYPE html>
<html lang="zh-cn">
    <head>
    <meta http-equiv="content-type" content="text/html;charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noodp"/>
    <meta name="author" content="神奇的战士">
    
    <meta name="keywords" content="blog, GNU/Linux, Linux, Ubuntu, Docker, Robot, Python, DRL">
    
    <link rel="prev" href="https://wangshub.github.io/posts/drl-stock/" />
    
    <link rel="canonical" href="https://wangshub.github.io/posts/how-ai-teach-robot-walk-01/" />
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>
        
        
            用 AI 教机器人走路 01：强化学习之连续动作控制环境 | 🍉 神奇的战士
        
    </title>
    <meta name="title" content="用 AI 教机器人走路 01：强化学习之连续动作控制环境 | 🍉 神奇的战士">
    
<link rel="stylesheet" href="/css/main.min.css">


    
    
 

<script type="application/ld+json">
{
    "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/wangshub.github.io"
    },
    "articleSection" : "posts",
    "name" : "用 AI 教机器人走路 01：强化学习之连续动作控制环境",
    "headline" : "用 AI 教机器人走路 01：强化学习之连续动作控制环境",
    "description" : "网络上关于强化学习的资料很多，但是针对双足机器人运动控制的强化学习的入门资料很少，所以打算将自己的理解汇总成一个博客系列。 关于深度强化学习 行",
    "inLanguage" : "zh-cn",
    "author" : "神奇的战士",
    "creator" : "神奇的战士",
    "publisher": "神奇的战士",
    "accountablePerson" : "神奇的战士",
    "copyrightHolder" : "神奇的战士",
    "copyrightYear" : "2020",
    "datePublished": "2020-12-31 15:34:16 \u002b0800 CST",
    "dateModified" : "2020-12-31 15:34:16 \u002b0800 CST",
    "url" : "https:\/\/wangshub.github.io\/posts\/how-ai-teach-robot-walk-01\/",
    "wordCount" : "1380",
    "keywords" : [ "AI","Reinforcement Learning","Robot", "🍉 神奇的战士"]
}
</script>

  </head>
    <body class="">
        <div class="wrapper">
            <nav class="navbar">
    
        <progress class="content_progress" max="0" value="0"></progress>
    
    <div class="container">
        
            <div class="navbar-header header-back2home-logo">
                <span class="logo_mark" >>$</span>
                <a href="https://wangshub.github.io">
                    <span class="logo_text" >cd /home/</span>
                    <span class="logo_cursor" ></span>
                </a>
            </div>
        
        <div class="navbar-right">
                
                <span class="menu">
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
                <span class="divide"></span>
                <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-dark-mode"></i></a>
                </span>
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
    
        <progress class="content_progress" max="0" value="0"></progress>
    
     <div class="container">
        <div class="navbar">
            <div class="navbar-header header-logo">
                    <a href="https://wangshub.github.io">🍉 神奇的战士</a>
            </div>
            <div class="navbar-right">
                <div><a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-dark-mode"></i></a></div>
                <div class="menu-toggle">
                    <span></span><span></span><span></span>
                </div>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                <nav class="mb-md">
                    
                    
                        <a class="menu-item" href="/posts/" title="">
                            <h3>Blog</h3>
                            <div class="menu-active"></div>
                        </a>
                    
                        <a class="menu-item" href="/tags/" title="">
                            <h3>Tags</h3>
                            <div class="menu-active"></div>
                        </a>
                    
                        <a class="menu-item" href="/about/" title="">
                            <h3>About</h3>
                            <div class="menu-active"></div>
                        </a>
                    
                </nav>
        </div>
    </div>
</nav>
            <main class="main">
                <div class="container">
                    
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">用 AI 教机器人走路 01：强化学习之连续动作控制环境</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://wangshub.github.io" rel="author">神奇的战士</a> with ♥
                <span class="post-time">
                on <time datetime=2020-12-31 itemprop="datePublished">December 31, 2020</time>
                </span>
                in
                
                <span class="post-word-count">1380 words</span>
        </div>
    </header>

    <div class="post-content">
        

        
        

        
        
        
        
        

        
        
        

        <p>网络上关于强化学习的资料很多，但是针对双足机器人运动控制的强化学习的入门资料很少，所以打算将自己的理解汇总成一个博客系列。</p>
<h2 id="关于深度强化学习">关于深度强化学习</h2>
<p>行走对于普通人来说是小事一桩，但是对机器人来说绝非易事。双足机器人是一个高维、非线性的复杂系统，在相对容易地环境中（如平坦地面行走）已有像 <a href="https://thinkhard.tech/posts/%E5%8F%8C%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%A6%82%E4%BD%95%E8%A1%8C%E8%B5%B0/">线性倒立摆模型</a> 等成熟的计算规划机器人的运动轨迹，再使用逆运动学等方法计算关节角度等控制参数，从而驱动机器类人行走。</p>
<p>但是在各种复杂环境（如崎岖路面、外部扰动、机器人协作等）下令机器人走起来仍然困难重重，比如突然踹机器人一脚、脚底打滑或者腿瘸了，简单的模型无法理解这些变化，深度网络恰好可以帮助机器人理解这些复杂环境，其中 <a href="https://zh.wikipedia.org/zh-hans/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">深度强化学习</a> 受到越来越多的关注。</p>
<p>在强化学习中环境通常被规范为马尔可夫决策过程，机器人需要根据当前环境的<strong>状态</strong>采取最佳的<strong>动作</strong>，以获得最高的回合奖励值。超级玛丽一类的强化学习环境中，角色在每个 Step 要采取<code>前进、后退、跳、发射子弹</code>等离散的动作。但双足行走机器人要在每个 Step 控制每个关节的角度或者扭矩等值驱动自身运动，所以输出的动作值为连续的浮点数，即<strong>连续运动控制</strong>。</p>
<p>连续动作控制环境的训练要比离散动作控制环境要更为困难，就算使用目前市面上配置最高的家用 CPU 和 GPU，对于训练像双足机器人行走、运动等复杂的连续运动控制环境，仍需要好几天的时间才能收敛。在 <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Alexirpan</a> 的博文中，他总结了深度强化学习一些问题：</p>
<ol>
<li>样本利用率非常低；</li>
<li>最终表现不够好，经常比不过基于模型的方法；</li>
<li>好的奖励函数难以设计；</li>
<li>难以平衡“探索”和“利用”, 以致算法陷入局部极小；</li>
<li>对环境的过拟合；</li>
<li>灾难性的不稳定性；</li>
<li>&hellip;</li>
</ol>
<p>这些问题的存在，在看论文或者算法过程中，常常陷入局部细节当。<a href="https://spinningup.openai.com/en/latest/spinningup/spinningup.html#learn-by-doing">OpenAI spinningup 博客</a> 提供了解决方案，学习 DRL 需要：</p>
<ul>
<li>从了解和利用现有的强化学习算法为起点，不要陷入局部细节；</li>
<li>在最简单的环境试验算法的有效性，快速迭代；</li>
<li>当试验成功后，再应用到复杂的环境中去；</li>
</ul>
<p>所以在后面内容中使用经典的 CartPole 环境作为演示。</p>
<h2 id="环境安装">环境安装</h2>
<p>目前 <a href="https://github.com/openai/baselines">baselines</a>、<a href="https://github.com/DLR-RM/stable-baselines3https://github.com/DLR-RM/stable-baselines3">stable-baselines3</a>、<a href="https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview">RLlib</a> 等强化学习库都内置了主流的强化学习算法，这里使用 RLlib 作为演示。</p>
<p>参考 <a href="https://docs.ray.io/en/master/rllib.html#running-rllib">Running RLlib</a> 安装步骤。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">pip install <span style="color:#e6db74">&#39;ray[rllib]&#39;</span>
pip install PyBullet
pip install gym
</code></pre></div><h2 id="cartpole-连续运动控制">CartPole 连续运动控制</h2>
<p>倒立摆小车 Cartpole 通常作为 DRL 算法的入门环境，但如下实验代码所示，在 Gym 内置环境 <code>CartPoleBulletEnv-v1</code> 下是离散的动作空间，策略只能根据环境状态让小车<code>向左、向右</code>移动</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> gym
env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;CartPoleBulletEnv-v1&#39;</span>)
<span style="color:#66d9ef">print</span>(env<span style="color:#f92672">.</span>observation_space)
<span style="color:#66d9ef">print</span>(env<span style="color:#f92672">.</span>action_space)

<span style="color:#75715e"># Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)</span>
<span style="color:#75715e"># Discrete(2)</span>
</code></pre></div><p>但是在 Pybullet 环境中，可以找到倒立摆小车的连续运动空间的试验环境 <code>CartPoleContinuousBulletEnv-v0</code>。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> gym
<span style="color:#f92672">import</span> pybullet_envs
env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;CartPoleContinuousBulletEnv-v0&#39;</span>)
<span style="color:#66d9ef">print</span>(env<span style="color:#f92672">.</span>observation_space)
<span style="color:#66d9ef">print</span>(env<span style="color:#f92672">.</span>action_space)

<span style="color:#75715e"># Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)</span>
<span style="color:#75715e"># Box(-10.0, 10.0, (1,), float32)</span>
</code></pre></div><h3 id="训练代码">训练代码</h3>
<p>训练代码如下，根据运行环境的配置，相应调节 <code>num_workers</code>、 <code>num_gpus</code>等参数。训练结果默认保存在 <code>~/ray_results</code> 目录下。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> gym
<span style="color:#f92672">import</span> pybullet_envs
<span style="color:#f92672">import</span> ray
<span style="color:#f92672">from</span> ray <span style="color:#f92672">import</span> tune
<span style="color:#f92672">from</span> ray.rllib.agents.ppo <span style="color:#f92672">import</span> PPOTrainer
<span style="color:#f92672">from</span> ray.tune.registry <span style="color:#f92672">import</span> register_env

ray<span style="color:#f92672">.</span>shutdown()
ray<span style="color:#f92672">.</span>init(ignore_reinit_error<span style="color:#f92672">=</span>True)

ENV <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;CartPoleContinuousBulletEnv-v0&#39;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">make_env</span>(env_config):
    <span style="color:#f92672">import</span> pybullet_envs
    <span style="color:#66d9ef">return</span> gym<span style="color:#f92672">.</span>make(ENV)
register_env(ENV, make_env)
TARGET_REWARD <span style="color:#f92672">=</span> <span style="color:#ae81ff">199</span>
TRAINER <span style="color:#f92672">=</span> PPOTrainer

config_train <span style="color:#f92672">=</span> {
        <span style="color:#e6db74">&#34;env&#34;</span>: ENV,
        <span style="color:#e6db74">&#34;num_workers&#34;</span>: <span style="color:#ae81ff">1</span>,
        <span style="color:#e6db74">&#34;num_gpus&#34;</span>: <span style="color:#ae81ff">0</span>,
        <span style="color:#e6db74">&#34;monitor&#34;</span>: True,
        <span style="color:#e6db74">&#34;evaluation_num_episodes&#34;</span>: <span style="color:#ae81ff">50</span>,
    }

analysis <span style="color:#f92672">=</span> tune<span style="color:#f92672">.</span>run(
    TRAINER,
    stop<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;episode_reward_mean&#34;</span>: TARGET_REWARD},
    config<span style="color:#f92672">=</span>config_train,
    checkpoint_at_end<span style="color:#f92672">=</span>True
)
</code></pre></div><p>使用 <a href="https://docs.ray.io/en/master/tune/index.html">Tune</a> 运行 RLlib 的 <code>TRAINER</code>，可以比较方便的管理实验和可视化。<code>tune.run()</code> 返回了 <code>ExperimentAnalysis</code> 类，可用于保存结果和加载 checkpoint。</p>
<h3 id="加载训练好的模型">加载训练好的模型</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">agent_test <span style="color:#f92672">=</span> PPOTrainer(config<span style="color:#f92672">=</span>config_train, env<span style="color:#f92672">=</span>ENV)
agent_test<span style="color:#f92672">.</span>restore(<span style="color:#e6db74">&#39;path/to/checkpoint/file&#39;</span>)
</code></pre></div><h3 id="测试回放">测试回放</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># instantiate env class</span>
env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(ENV)

<span style="color:#75715e"># run until episode ends</span>
episode_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
done <span style="color:#f92672">=</span> False
obs <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
<span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
    action <span style="color:#f92672">=</span> agent_test<span style="color:#f92672">.</span>compute_action(obs)
    obs, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
    episode_reward <span style="color:#f92672">+=</span> reward

<span style="color:#66d9ef">print</span>(episode_reward)

<span style="color:#75715e"># &gt;&gt; 200.0</span>
</code></pre></div><h3 id="学习曲线">学习曲线</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> pandas
df <span style="color:#f92672">=</span> pandas<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;/your/path/to/progress.csv&#39;</span>)
df<span style="color:#f92672">.</span>plot(kind<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;line&#39;</span>, x<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;timesteps_total&#39;</span>, y<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;episode_reward_mean&#39;</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">4</span>))
</code></pre></div><img src ="img/2021-01-12-17-22-20.png" width="60%"/>
<h2 id="结语">结语</h2>
<p>博客大致记录了下 <code>rllib</code> 在连续的 Pybullet 仿真环境下的使用流程，后续将记录下如何使用 Pybullet 内的连续控制的强化学习环境。</p>

    </div>

    <div class="post-copyright">
            
            <p class="copyright-item">
                <span>Author:</span>
                <span>神奇的战士 </span>
                </p>
            

            
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://wangshub.github.io/posts/how-ai-teach-robot-walk-01/>https://wangshub.github.io/posts/how-ai-teach-robot-walk-01/</span>
            </p>
            
            
            <p class="copyright-item lincese">
                本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
            </p>
            
    </div>


    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s):
            
            <span class="tag"><a href="https://wangshub.github.io/tags/ai/">
                    #AI</a></span>
            
            <span class="tag"><a href="https://wangshub.github.io/tags/reinforcement-learning/">
                    #Reinforcement Learning</a></span>
            
            <span class="tag"><a href="https://wangshub.github.io/tags/robot/">
                    #Robot</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> ·
                <span><a href="https://wangshub.github.io">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://wangshub.github.io/posts/drl-stock/" class="prev" rel="prev" title="如何用深度强化学习模拟炒股"><i class="iconfont icon-left"></i>&nbsp;如何用深度强化学习模拟炒股</a>
        
        
    </div>

    <div class="post-comment">
        
            
        
    </div>
</article>
                </div>
            </main>
            <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2016 - 2021</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i>
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://wangshub.github.io">神奇的战士</a> | </span>
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/Mogeko/Mogege" target="_blank" rel="external nofollow">Mogege</a></span>
    </div>
</footer>






<script defer src="/js/vendor_main.min.js"></script>







<script src="https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script> pangu.spacingPage();</script>



<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>

        </div>
    </body>
</html>
