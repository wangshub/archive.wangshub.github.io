<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta name=author content="神奇的战士"><link rel=prev href=https://wangshub.github.io/posts/mlpgaussianpolicy/><link rel=next href=https://wangshub.github.io/posts/%E7%94%A8python%E8%8E%B7%E5%8F%96b%E7%AB%99%E6%92%AD%E6%94%BE%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95/><link rel=canonical href=https://wangshub.github.io/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title>强化学习:策略梯度算法 | 🍉 神奇的战士</title><meta name=title content="强化学习:策略梯度算法 | 🍉 神奇的战士"><link rel=stylesheet href=/css/main.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/wangshub.github.io"},"articleSection":"posts","name":"强化学习:策略梯度算法","headline":"强化学习:策略梯度算法","description":"强化学习:策略梯度算法 策略梯度的公式推导 ​ 学习参数化表示的策略 (Parameterized policy), 输入环境状态 $ S $ 来选择动作 $a$ ，这里使用 $\\theta \\in \\mathbb { R } ^ { d }$ 来表示策略的参","inLanguage":"zh-cn","author":"神奇的战士","creator":"神奇的战士","publisher":"神奇的战士","accountablePerson":"神奇的战士","copyrightHolder":"神奇的战士","copyrightYear":"2019","datePublished":"2019-01-15 19:46:17 \u002b0000 UTC","dateModified":"2019-01-15 19:46:17 \u002b0000 UTC","url":"https:\/\/wangshub.github.io\/posts\/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95\/","wordCount":"1877","keywords":["Algorithm","Python","🍉 神奇的战士"]}</script></head><body><div class=wrapper><nav class=navbar><progress class=content_progress max=0 value=0></progress><div class=container><div class="navbar-header header-back2home-logo"><span class=logo_mark>>$</span>
<a href=https://wangshub.github.io><span class=logo_text>cd /home/</span>
<span class=logo_cursor></span></a></div><div class=navbar-right><span class=menu><a class=menu-item href=/posts/>Blog</a>
<a class=menu-item href=/categories/>Categories</a>
<a class=menu-item href=/tags/>Tags</a>
<a class=menu-item href=/about/>About</a>
<span class=divide></span><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-dark-mode"></i></a></span></div></div></nav><nav class=navbar-mobile id=nav-mobile style=display:none><progress class=content_progress max=0 value=0></progress><div class=container><div class=navbar><div class="navbar-header header-logo"><a href=https://wangshub.github.io>🍉 神奇的战士</a></div><div class=navbar-right><div><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-dark-mode"></i></a></div><div class=menu-toggle><span></span><span></span><span></span></div></div></div><div class=menu id=mobile-menu><nav class=mb-md><a class=menu-item href=/posts/><h3>Blog</h3><div class=menu-active></div></a><a class=menu-item href=/categories/><h3>Categories</h3><div class=menu-active></div></a><a class=menu-item href=/tags/><h3>Tags</h3><div class=menu-active></div></a><a class=menu-item href=/about/><h3>About</h3><div class=menu-active></div></a></nav></div></div></nav><main class=main><div class=container><article class=post-warp itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline">强化学习:策略梯度算法</h1><div class=post-meta>Written by <a itemprop=name href=https://wangshub.github.io rel=author>神奇的战士</a> with ♥
<span class=post-time>on <time datetime=2019-01-15 itemprop=datePublished>January 15, 2019</time></span>
in
<span class=post-word-count>1877 words</span></div></header><div class=post-content><h1 id=强化学习策略梯度算法>强化学习:策略梯度算法</h1><h2 id=策略梯度的公式推导>策略梯度的公式推导</h2><p><img src=https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190115200214.png alt></p><p>​ 学习<strong>参数化表示的策略</strong> (Parameterized policy), 输入环境状态 $ S $ 来选择动作 $a$ ，这里使用 $\theta \in \mathbb { R } ^ { d }$ 来表示策略的参数向量，因此策略函数表示为</p><p>$$\pi ( a | s , \boldsymbol { \theta } ) = \operatorname { P_r } \left{ A _ { t } = a | S _ { t } = s , \boldsymbol { \theta } _ { t } = \boldsymbol { \theta } \right} \tag{1} $$</p><p>其中时刻 $t$ ，环境状态为 $s$ ，参数为 $\theta$ ，输出动作 $a$ 的概率为 $P_r$</p><p>因此生成马尔可夫决策过程的一个<em>轨迹</em>（trajectory）$\tau = (\mathbf { s } _ { 1 } , \mathbf { a } _ { 1 } , \dots , \mathbf { s } _ { T } , \mathbf { a } _ { T })$ 的概率为</p><p>$$\underbrace { p _ { \theta } \left( \mathbf { s } _ { 1 } , \mathbf { a } _ { 1 } , \ldots , \mathbf { s } _ { T } , \mathbf { a } _ { T } \right) } _ { \pi _ { \theta } ( \tau ) } = p \left( \mathbf { s } _ { 1 } \right) \prod _ { t = 1 } ^ { T } \pi _ { \theta } \left( \mathbf { a } _ { t } | \mathbf { s } _ { t } \right) p \left( \mathbf { s } _ { t + 1 } | \mathbf { s } _ { t } , \mathbf { a } _ { t } \right) \tag{2} $$</p><p>更一般地，将策略 $\pi$ 下生成轨迹 $\tau$ 的概率表示为</p><p>$$P ( \tau | \pi ) = \rho _ { 1 } \left( s _ { 1 } \right) \prod _ { t = 0 } ^ { T } P \left( s _ { t + 1 } | s _ { t } , a _ { t } \right) \pi \left( a _ { t } | s _ { t } \right) \tag{3}$$</p><p>​ 策略梯度方法的目标就是找到一组最佳的参数 $\theta ^ { \star }$ 来表示策略函数使得累计奖励的期望最大，即</p><p>$$\theta ^ { \star } = \arg \max _ { \theta } E _ { \tau \sim p _ { \theta } ( \tau ) } \left[ \sum _ { t } r \left( \mathbf { s } _ { t } , \mathbf { a } _ { t } \right) \right] \tag{4}$$</p><p>​ 令累积奖励为 $R ( \tau ) = \sum _ { t = 1 } ^ { T } r \left( s _ { t } , a _ { t } \right)$ ，设定优化目标 $J\left( \pi _ { \theta } \right)$ 优化策略参数使得奖励的期望值最大</p><p>$$J\left( \pi _ { \theta } \right) = \underset { \tau \sim \pi _ { \theta } } { \mathrm { E } } [ R ( \tau ) ]\tag{5} $$</p><p>对 $J\left( \pi _ { \theta } \right)$ 求梯度可得策略梯度 $\nabla _ { \theta } J ( \theta ) $ ，公式 (6) 的推导过程请参见<a href=https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient>链接</a></p><p>$$\begin{aligned} \nabla _ { \theta } J ( \theta ) &= \int \nabla _ { \theta } \pi _ { \theta } ( \tau ) r ( \tau ) d \tau \&= \int \pi _ { \theta } ( \tau ) \nabla _ { \theta } \log \pi _ { \theta } ( \tau ) r ( \tau ) d \tau \ &= E _ { \tau \sim \pi _ { \theta } ( \tau ) } \left[ \nabla _ { \theta } \log \pi _ { \theta } ( \tau ) r ( \tau ) \right]\end{aligned} \tag{6}$$</p><p>将策略 (1) 两边取 log 对数，然后带入梯度表达式 (6) ，推导策略梯度的公式请参考下图</p><p><img src=https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117080942.png alt></p><p>根据策略 $\pi _ { \theta }$ 生成 $N$ 条轨迹如图所示</p><p><img src=https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117084618.png alt></p><p>利用上图 $N$ 条轨迹的经验平均对策略梯度进行逼近，有公式 (7) (8)</p><p>$$J ( \theta ) = E _ { \tau \sim p _ { \theta } ( \tau ) } \left[ \sum _ { t } r \left( \mathbf { s } _ { t } , \mathbf { a } _ { t } \right) \right] \approx \frac { 1 } { N } \sum _ { i } \sum _ { t } r \left( \mathbf { s } _ { i , t } , \mathbf { a } _ { i , t } \right) \tag{7}$$</p><p>$$\nabla _ { \theta } J ( \theta ) \approx \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left( \sum _ { t = 1 } ^ { T } \nabla _ { \theta } \log \pi _ { \theta } \left( \mathbf { a } _ { i , t } | \mathbf { s } _ { i , t } \right) \right) \left( \sum _ { t = 1 } ^ { T } r \left( \mathbf { s } _ { i , t } , \mathbf { a } _ { i , t } \right) \right) \tag{8}$$</p><p>其中 $N$ 为轨迹的数量，$T$ 为一条轨迹的长度，假设已知策略 $\pi _ { \theta }$ ，那么就可以计算出策略的梯度 $\nabla _ { \theta } \log \pi _ { \theta } ( a | s )$。另一方面，根据策略 $\pi _ { \theta }$ ，在仿真环境 $E$ 中生成 $N$ 条轨迹的数据，即可计算出 (8)，根据梯度上升 对参数 $\theta$ 进行一步更新，如公式 (9)</p><p>$$
\theta \leftarrow \theta + \alpha \nabla _ { \theta } J ( \theta ) \tag{9}
$$</p><p>总结下来就是：</p><ul><li>增加带来正激励的概率</li><li>减少带来负激励的概率</li></ul><img src=https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117203920.png width=60%/><h2 id=策略梯度蒙特卡罗-reinforce-算法>策略梯度蒙特卡罗 REINFORCE 算法</h2><p>根据公式 (7) (8) (9) 可得<strong>蒙特卡罗 REINFORCE 算法</strong>流程</p><p><img src=https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117091205.png alt=公式 loading=lazy></p><p>写成伪代码形式</p><p><img src=https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117091717.png alt=伪代码 loading=lazy></p><h2 id=example-高斯策略梯度算法>Example: 高斯策略梯度算法</h2><p>策略属于概率分布，可以用神经网络来表示这种概率分布，输入状态 $s$ ，神经网络将 $s$ 映射成向量 $μ$，然后网络输出概率 $p ( a | \mu )$ 和动作采样值 $a \sim p ( a | \mu )$，令 $r$ 为 log 标准差。</p><p>$$
\mathcal { N } \left( \text { mean } = \text { NeuralNet } \left( s ; \left{ W _ { i } , b _ { i } \right} _ { i = 1 } ^ { L } \right) , stdev = exp(r) \right)
$$</p><p>其中 $\mu = [ \text { mean, stdev } ]$</p><img src=https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117204712.png><p>在连续的运动空间中，通常使用<strong>高斯策略</strong>，假设方差为 $\sigma ^ { 2 }$ ，策略是高斯的，输入状态 $s$ 输出动作 $a$ 服从 $a \sim \mathcal { N } \left( \mu ( s ) , \sigma ^ { 2 } \right)$，那么 log 策略梯度为</p><p>$$
\nabla _ { \theta } \log \pi _ { \theta } ( s , a ) = \frac { ( a - \mu ( s ) ) \phi ( s ) } { \sigma ^ { 2 } }
\tag{10}$$</p><p>在实际使用高斯策略时，用神经网络来表示，即令 $f _ { neural\ network } \left( \mathbf { s } _ { t } \right) = \mu ( s _ t )$，那么策略 $\pi _ \theta$</p><p>$$\log \pi _ { \theta } \left( \mathbf { a } _ { t } | \mathbf { s } _ { t } \right) = - \frac { 1 } { 2 } \left| f \left( \mathbf { s } _ { t } \right) - \mathbf { a } _ { t } \right| _ { \Sigma } ^ { 2 } + const \tag{11}$$</p><p>策略的梯度</p><p>$$\nabla _ { \theta } \log \pi _ { \theta } \left( \mathbf { a } _ { t } | \mathbf { s } _ { t } \right) = - \frac { 1 } { 2 } \Sigma ^ { - 1 } \left( f \left( \mathbf { s } _ { t } \right) - \mathbf { a } _ { t } \right) \frac { d f } { d \theta } \tag{12}$$</p><p>然后反向传播，更新网络参数</p><p>$$- \frac { 1 } { 2 } \Sigma ^ { - 1 } \left( f \left( \mathbf { s } _ { t } \right) - \mathbf { a } _ { t } \right) \left( \sum _ { t } r \left( \mathbf { s } _ { t } , \mathbf { a } _ { t } \right) \right) \tag{13}$$</p><h2 id=参考链接>参考链接</h2><ul><li><a href=https://hadovanhasselt.files.wordpress.com/2016/01/pg1.pdf>Lecture 8: Policy Gradient</a></li><li><a href=https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html>Policy Gradient Algorithms</a></li><li><a href=http://incompleteideas.net/book/the-book-2nd.html>Reinforcement Learning: An Introduction</a></li></ul></div><div class=post-copyright><p class=copyright-item><span>Author:</span>
<span>神奇的战士</span></p><p class=copyright-item><span>Link:</span>
<a href=https://wangshub.github.io/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/>https://wangshub.github.io/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/</span></p></div><div class=post-tags><section><i class="iconfont icon-tag"></i>Tag(s):
<span class=tag><a href=https://wangshub.github.io/tags/algorithm/>#Algorithm</a></span>
<span class=tag><a href=https://wangshub.github.io/tags/python/>#Python</a></span></section><section><a href=javascript:window.history.back();>back</a></span> ·
<span><a href=https://wangshub.github.io>home</a></span></section></div><div class=post-nav><a href=https://wangshub.github.io/posts/mlpgaussianpolicy/ class=prev rel=prev title=MLPGaussianPolicy><i class="iconfont icon-left"></i>&nbsp;MLPGaussianPolicy</a>
<a href=https://wangshub.github.io/posts/%E7%94%A8python%E8%8E%B7%E5%8F%96b%E7%AB%99%E6%92%AD%E6%94%BE%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95/ class=next rel=next title=用Python获取B站播放历史记录>用Python获取B站播放历史记录&nbsp;<i class="iconfont icon-right"></i></a></div><div class=post-comment></div></article></div></main><footer class=footer><div class=copyright>&copy;
<span itemprop=copyrightYear>2016 - 2020</span>
<span class=with-love><i class="iconfont icon-love"></i></span><span class=author itemprop=copyrightHolder><a href=https://wangshub.github.io>神奇的战士</a> |</span>
<span>Powered by <a href=https://gohugo.io/ target=_blank rel="external nofollow">Hugo</a> & <a href=https://github.com/Mogeko/Mogege target=_blank rel="external nofollow">Mogege</a></span></div></footer><script defer src=/js/vendor_main.min.js></script><script src=https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin=anonymous></script><script>pangu.spacingPage();</script><script>MathJax={tex:{inlineMath:[["$","$"]],},displayMath:[["$$","$$"],["\[\[","\]\]"],],svg:{fontCache:"global",},};</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></div></body></html>